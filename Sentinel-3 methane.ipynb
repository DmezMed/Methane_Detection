{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUG0zC2fOA2b"
      },
      "source": [
        "# Sentinel-3 data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6P7YkuMIVyi"
      },
      "source": [
        "## 0. Install and import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YNpdV1uk8M1"
      },
      "outputs": [],
      "source": [
        "!pip install cartopy\n",
        "!pip install netCDF4\n",
        "!pip install matplotlib-scalebar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-ZJS41Gs1tl"
      },
      "outputs": [],
      "source": [
        "# a Google Eath engine python API is needed.\n",
        "# See https://developers.google.com//earth-engine/tutorials/community/intro-to-python-api\n",
        "project_name = str(input('Earth engine project name: '))\n",
        "import ee ; ee.Authenticate() ; ee.Initialize(project=project_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1U8E3DpBBudz"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import os, time\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import pandas as pd\n",
        "from datetime import datetime as dt\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"./Scripts/\")\n",
        "from Sentinel3Class import sentinel3\n",
        "from Sentinel3_Utilities import (\n",
        "    read_full_slstr_data,\n",
        "    get_data_on_grid_of_interest,\n",
        "    get_specific_date_files,\n",
        "    perform_interpolation,\n",
        "    set_nan_inf_neg_to_zero,\n",
        "    get_inland_water_mask\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fm_dwfPa6nN"
      },
      "source": [
        "## 1. Inputs\n",
        "\n",
        "Creates an input dictionary that includes all inputs necesary for processing each plume.\n",
        "\n",
        "*   **plume_name**: name of the location, useful for filenaming for example\n",
        "*   **cen_lon, cen_lat**: central longitude and latitude of the scene that is to be considered.\n",
        "*   **delta_lonlat**: half of the width of the scene that is to be considered for interpolation\n",
        "*   **pixel_size**: size of the new pixels, default is 0.001, which was found to work well.\n",
        "*   **all_data_dir**: directory containg the interpolated data, both main and reference observations\n",
        "*   **specific_main_dates**: a list with datetime objects defining with dates are to be considered as main dates, that is potentially containing methane plumes. There are three ways to define the specific_main_dates\n",
        "\n",
        "The main dates can be specified in multiple ways through specific_main_dates:\n",
        "1.  A list containing datetime date objects (excluding time stamps), [dt(y, m, d).date()]\n",
        "\n",
        "        specific_main_dates = [dt(2020, 10, 11).date(), dt(2020, 10, 15).date()]\n",
        "\n",
        "2. From a CSV file, with 'Days', 'Months', 'Years' as keys \n",
        "\n",
        "        csv_file = '/data/maartenvn/Varon_Algeria/S2_detected_plume_dates.csv'\n",
        "        dates_df = pd.read_csv(csv_file)\n",
        "        days = dates_df['Days']\n",
        "        months = dates_df['Months']\n",
        "        years = dates_df['Years']\n",
        "        specific_main_dates = []\n",
        "        for i in range(len(days)):\n",
        "            specific_main_dates.append(dt(years[i], months[i], days[i]))\n",
        "\n",
        "3. 'All': all dates/observations within the all_data_dir directory\n",
        "\n",
        "        specific_main_dates = 'All'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_PP90Nt7bWL"
      },
      "outputs": [],
      "source": [
        "cen_lon = 6.17\n",
        "cen_lat = 31.86\n",
        "\n",
        "delta_lonlat = 0.2\n",
        "pixel_size = 0.001\n",
        "\n",
        "case_name = 'Case name' # Name of the data folder\n",
        "\n",
        "original_file_dir = '/Data/'+ case_name + '/'\n",
        "\n",
        "####################################\n",
        "source = '/Data/'\n",
        "####################################\n",
        "\n",
        "####################################\n",
        "plume_name = 'Plume 1'\n",
        "####################################\n",
        "all_data_dir = source + case_name + '_' + str(np.around(cen_lon, decimals=4)) + '_' + str(np.around(cen_lat, decimals=4)) + '_interpolated_' + str(pixel_size) + '_' + str(delta_lonlat) + '/'\n",
        "specific_main_dates = [#dt(2020, 1, 3).date(),\n",
        "                       dt(2020, 1, 4).date(),\n",
        "                       dt(2020, 1, 5).date(),\n",
        "                       dt(2020, 1, 6).date(),\n",
        "                       dt(2020, 1, 7).date(),\n",
        "                       #dt(2020, 1, 8).date(),\n",
        "                       #dt(2020, 1, 9).date()\n",
        "                        ]\n",
        "Plume1_input = {'plume_name': plume_name,\n",
        "                'cen_lon': cen_lon,\n",
        "                'cen_lat': cen_lat,\n",
        "                'delta_lonlat': delta_lonlat,\n",
        "                'pixel_size': pixel_size,\n",
        "                'all_data_dir': all_data_dir,\n",
        "                'specific_main_dates': specific_main_dates}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUJgghswWsI7"
      },
      "source": [
        "## 2. Interpolate\n",
        "\n",
        "Having downloaded the data, an interpolation, or re-gridding, is required to match the pixels of main and reference observations. This is done for only a portion of the complete scene that was downloaded. This time-consuming step is only required once, but it can be done multiple times for one case with different centers or scene widths (delta_lonlat) if the exact source location is unknown or further zooming is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgKb1h05a-FU"
      },
      "outputs": [],
      "source": [
        "# Interpolate all files to a regular grid\n",
        "\n",
        "interpolated_dir = all_data_dir\n",
        "\n",
        "if not os.path.isdir(interpolated_dir):\n",
        "    os.makedirs(interpolated_dir)\n",
        "\n",
        "files_to_interpolate = os.listdir(original_file_dir)\n",
        "files_to_interpolate.sort()\n",
        "print(delta_lonlat)\n",
        "\n",
        "####################################\n",
        "##           INTERPOLATE          ##\n",
        "####################################\n",
        "\n",
        "# Define area of interest\n",
        "grid_of_interest = [cen_lon-delta_lonlat, cen_lon+delta_lonlat,\n",
        "                    cen_lat-delta_lonlat, cen_lat+delta_lonlat]\n",
        "\n",
        "# Desired grid for interpolation later on\n",
        "lon_reg = np.arange(grid_of_interest[0], grid_of_interest[1]+pixel_size, pixel_size)\n",
        "lat_reg = np.arange(grid_of_interest[2], grid_of_interest[3]+pixel_size, pixel_size)\n",
        "lon_reg, lat_reg = np.meshgrid(lon_reg, lat_reg)\n",
        "\n",
        "print('Interpolating ', len(files_to_interpolate), ' files ... ')\n",
        "\n",
        "for file in files_to_interpolate:\n",
        "    lon, lat, s6, s5, s4, s3, s2, s1, landscape, sza, vza, lon_tx, lat_tx = read_full_slstr_data(original_file_dir + file)\n",
        "    lon, lat, [s6, s5, s4, s3, s2, s1, landscape] = get_data_on_grid_of_interest(lon, lat, [s6, s5, s4, s3, s2, s1, landscape], grid_of_interest)\n",
        "    water_mask = get_inland_water_mask(landscape)\n",
        "    lon_tx, lat_tx, [sza, vza] = get_data_on_grid_of_interest(lon_tx, lat_tx, [sza, vza], grid_of_interest)\n",
        "    sza_average = np.mean(sza)\n",
        "    vza_average = np.mean(vza)\n",
        "\n",
        "\n",
        "    try:\n",
        "        [s6, s5, s4, s3, s2, s1, water_mask] = perform_interpolation(lon, lat, lon_reg, lat_reg, [s6, s5, s4, s3, s2, s1, water_mask])\n",
        "    except:\n",
        "        print(f'Error in file {file}. Skipped')\n",
        "        continue\n",
        "\n",
        "    [s6, s5, s4, s3, s2, s1, water_mask] = set_nan_inf_neg_to_zero([s6, s5, s4, s3, s2, s1, water_mask])\n",
        "\n",
        "    # Save lon, lat, s6, s5, s4, s3, s2, s1 and water_mask to interpolated dir.\n",
        "    save_to_path = interpolated_dir + file[:-5]\n",
        "    if not os.path.isdir(save_to_path):\n",
        "                os.makedirs(save_to_path)\n",
        "    np.save(save_to_path + '/lon_interpolated.npy', lon_reg)\n",
        "    np.save(save_to_path + '/lat_interpolated.npy', lat_reg)\n",
        "    np.save(save_to_path + '/S6_interpolated.npy', s6)\n",
        "    np.save(save_to_path + '/S5_interpolated.npy', s5)\n",
        "    np.save(save_to_path + '/S4_interpolated.npy', s4)\n",
        "    np.save(save_to_path + '/S3_interpolated.npy', s3)\n",
        "    np.save(save_to_path + '/S2_interpolated.npy', s2)\n",
        "    np.save(save_to_path + '/S1_interpolated.npy', s1)\n",
        "    np.save(save_to_path + '/water_interpolated.npy', water_mask)\n",
        "    np.save(save_to_path + '/SZA_average.npy', sza_average)\n",
        "    np.save(save_to_path + '/VZA_average.npy', vza_average)\n",
        "\n",
        "print(' ... Finished interpolation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O9L5_1cH9-Y"
      },
      "source": [
        "## 3. Process\n",
        "\n",
        "### Settings:\n",
        "\n",
        "*   **auxiliary_bands_to_consider**: bands to consider in the auxiliary mask\n",
        "\n",
        "*   **correlation_minimum**: minimum auxiliary band correlation to consider it in the mask\n",
        "\n",
        "*   **delR_mask_limit**: maximum DelR_MBSP value to consider in that filter, as to remove already signals that cannot be due to methane\n",
        "\n",
        "*   **delta_lonlat_tropomi**: half width of tropomi scene\n",
        "\n",
        "*   **destripe**: whether to apply the destriping procedure\n",
        "\n",
        "*   **filter_auxiliary_mask**: whether to apply the structural similarity mask between auxiliary bands to identify noise regions\n",
        "\n",
        "*   **filter_delR_mask**: whether to apply the structural similarity mask between DelR_MBSP to identify regions of interest (potential methane plumes)\n",
        "\n",
        "*   **filter_median**: whether to include a median filter\n",
        "\n",
        "*   **filter_top_xperc**: whether to include a top x percentile filter\n",
        "\n",
        "*   **img_quality_S3**: quality of background in S3 standard image\n",
        "\n",
        "*   **input_dict**: dictionary containing the cases with their corresponding case specific inputs as defined in Inputs\n",
        "\n",
        "*   **median_filter_size**: size of median filter\n",
        "\n",
        "*   **min_delR**: minimum value of DelR_MBMP to consider the results valid\n",
        "\n",
        "*   **min_vis_corr**: minimum correlation with main observation in the visible band (called S1) to consider a reference observation\n",
        "\n",
        "*   **number_of_references_original**: the number of best reference observations that should be used to construct the final reference observations by taking the pixel-wise median. If not enough valid once are present, less will be used.\n",
        "\n",
        "*   **reference_day_range**: range of days ahead and before the main date that are considered valid reference observations. Optional: (defaults are present in Sentinel3Class.py)\n",
        "\n",
        "*   **save_results**: whether to save the standard images and Pickle files\n",
        "\n",
        "*   **SHOW_STANDARD_S3**: whether to show the Sentinel 3 standard image\n",
        "\n",
        "*   **top_xperc**: the value for x in the filter\n",
        "\n",
        "*   **VERBOSE**: whether to print information during the processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulA9oPLiFGoU"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "####################################\n",
        "##   DEFINE PROCESSING SETTINGS   ##\n",
        "####################################\n",
        "\n",
        "##### Reference date selection criteria -> optional parameters when initializing sentinel3() object\n",
        "min_vis_corr = 0.3\n",
        "min_delR = -5.\n",
        "number_of_references_original = 5\n",
        "reference_day_range = 30\n",
        "\n",
        "##### Filter settings\n",
        "\n",
        "destripe = True\n",
        "\n",
        "filter_delR_mask = True\n",
        "delR_mask_limit = 0.1\n",
        "\n",
        "filter_auxiliary_mask = True\n",
        "correlation_minimum = 0.5\n",
        "auxiliary_bands_to_consider = [1, 2, 3] # 0=S4, 1=S3, 2=S2, 3=S1\n",
        "\n",
        "filter_top_xperc = False\n",
        "top_xperc = 0\n",
        "\n",
        "filter_median = False\n",
        "median_filter_size = 5\n",
        "\n",
        "####################################\n",
        "##      SET DESIRED RESULTS       ##\n",
        "####################################\n",
        "\n",
        "SHOW_STANDARD_S3 = True\n",
        "\n",
        "save_results = True\n",
        "\n",
        "img_quality_S3 = 14\n",
        "\n",
        "VERBOSE = True\n",
        "\n",
        "####################################\n",
        "##         LOAD CASE INPUTS       ##\n",
        "####################################\n",
        "\n",
        "input_dict = {\n",
        "    'Plume1': Plume1_input,\n",
        "     }\n",
        "\n",
        "########################################\n",
        "# Create output folders\n",
        "\n",
        "if save_results:\n",
        "  if not os.path.isdir('Pickle_files/Sentinel_3'):\n",
        "      os.makedirs('Pickle_files/Sentinel_3')\n",
        "      os.makedirs('Pickle_files/Sentinel_2')\n",
        "  if not os.path.isdir('StandardImages/Sentinel_3'):\n",
        "      os.makedirs('StandardImages/Sentinel_3')\n",
        "      os.makedirs('StandardImages/Sentinel_2')\n",
        "\n",
        "all_omegas = []\n",
        "all_filtered_omegas = []\n",
        "\n",
        "# Loop over the locations\n",
        "for key in input_dict.keys():\n",
        "\n",
        "    if VERBOSE:\n",
        "        print();print('###################################################');print()\n",
        "        print(key)\n",
        "        print();print('###################################################');print()\n",
        "\n",
        "    plume_name = input_dict[key]['plume_name']\n",
        "    cen_lon = input_dict[key]['cen_lon']\n",
        "    cen_lat = input_dict[key]['cen_lat']\n",
        "    delta_lonlat = input_dict[key]['delta_lonlat']\n",
        "    pixel_size = input_dict[key]['pixel_size']\n",
        "    all_data_dir = input_dict[key]['all_data_dir']\n",
        "    specific_main_dates = input_dict[key]['specific_main_dates']\n",
        "\n",
        "    if VERBOSE:\n",
        "        print(all_data_dir)\n",
        "\n",
        "    if specific_main_dates == 'All':\n",
        "        if VERBOSE:\n",
        "            print('Processing all dates')\n",
        "        main_file_list = os.listdir(all_data_dir)\n",
        "        for i in range(len(main_file_list)):\n",
        "          main_file_list[i] = all_data_dir + main_file_list[i]\n",
        "\n",
        "    else:\n",
        "        if VERBOSE:\n",
        "            print('Processing specific dates only')\n",
        "        main_file_list = get_specific_date_files(specific_main_dates, all_data_dir)\n",
        "\n",
        "    # Loop over main date files of current location\n",
        "    for main_file in main_file_list:\n",
        "\n",
        "        S3_object = sentinel3(plume_name,\n",
        "                              all_data_dir,\n",
        "                              main_file,\n",
        "                              cen_lon,\n",
        "                              cen_lat,\n",
        "                              delta_lonlat=delta_lonlat,\n",
        "                              pixel_size=pixel_size,\n",
        "                              verbose=VERBOSE,\n",
        "                              number_of_references_original=number_of_references_original,\n",
        "                              reference_day_range=reference_day_range\n",
        "                             )\n",
        "\n",
        "        S3_object.get_correlation_based_reference_data()\n",
        "        S3_object.perform_multi_band_multi_pass_method()\n",
        "        S3_object.apply_filters_and_calculate_omega(destripe=destripe,\n",
        "                                                    filter_delR_mask=filter_delR_mask,\n",
        "                                                    delR_mask_limit=delR_mask_limit,\n",
        "                                                    filter_auxiliary_mask=filter_auxiliary_mask,\n",
        "                                                    correlation_minimum=correlation_minimum,\n",
        "                                                    auxiliary_bands_to_consider=auxiliary_bands_to_consider,\n",
        "                                                    filter_top_xperc=filter_top_xperc,\n",
        "                                                    top_xperc=top_xperc,\n",
        "                                                    filter_median=filter_median,\n",
        "                                                    median_filter_size=median_filter_size\n",
        "                                                   )\n",
        "\n",
        "        S3_object.create_standard_images_and_save_data_to_pickle_files(S3_img_quality=img_quality_S3,\n",
        "                                                                           S3_show=SHOW_STANDARD_S3,\n",
        "                                                                           retrieve_tropomi=False,\n",
        "                                                                           retrieve_S2=False,\n",
        "                                                                           save_results=save_results)\n",
        "\n",
        "        all_omegas.append(S3_object.omega)\n",
        "        all_filtered_omegas.append(S3_object.omega_filtered)\n",
        "\n",
        "########################################################################\n",
        "# END\n",
        "########################################################################\n",
        "\n",
        "end_time = time.time()\n",
        "print()\n",
        "print('###')\n",
        "print()\n",
        "print('Script duration: ', (end_time-start_time), ' seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sum_omega = np.sum(all_omegas, axis=0)\n",
        "sum_filtered_omega = np.sum(all_filtered_omegas, axis=0)\n",
        "\n",
        "S3_object.create_single_plot(sum_omega, sum_filtered_omega, 'Total methane emitted')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Z4XF9BcvQi8T",
        "4fm_dwfPa6nN",
        "KUJgghswWsI7"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
